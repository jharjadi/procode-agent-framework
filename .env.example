# LLM Configuration - Set at least one API key to enable LLM-based intent classification
# The agent will auto-detect and use the first available provider in this order:
# 1. Ollama (Local, FREE) - Best for cost optimization
# 2. Anthropic (Claude 3 Haiku) - Cheap cloud option (12x cheaper than Sonnet)
# 3. Google (Gemini Flash-8B) - Cheap cloud option (2x cheaper than Flash)
# 4. OpenAI (GPT-4o-mini) - Already cost-optimized

# === COST-OPTIMIZED OPTIONS ===

# Option 1: Ollama (Local, FREE) - Recommended for development
# Install: brew install ollama
# Start: ollama serve
# Pull model: ollama pull llama3.2:3b
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.2:3b

# Option 2: Anthropic Claude 3 Haiku (12x cheaper than Sonnet)
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Option 3: Google Gemini Flash-8B (2x cheaper than Flash)
# GOOGLE_API_KEY=your-google-api-key-here
# GOOGLE_MODEL=gemini-1.5-flash-8b

# Option 4: OpenAI GPT-4o-mini (already cost-optimized)
# OPENAI_API_KEY=your-openai-api-key-here

# === MULTI-LLM STRATEGY SETTINGS ===

# Force specific provider for intent classification (optional)
# INTENT_LLM_PROVIDER=ollama  # or "anthropic", "google", "openai"
# LLM_PROVIDER=anthropic  # Fallback for backward compatibility

# Confidence threshold for deterministic classifier (0.0-1.0)
# Higher = fewer LLM calls = more cost savings
# CONFIDENCE_THRESHOLD=0.8  # Default: 80% confidence required

# Enable caching of intent classifications
# ENABLE_INTENT_CACHE=true

# Enable/disable LLM intent classification (default: true if any API key is set)
# USE_LLM_INTENT=true

# Tool Configuration
# Enable real GitHub API integration (default: false, uses mocked tools)
# USE_REAL_TOOLS=false
# GITHUB_TOKEN=your-github-token-here
# GITHUB_REPO=owner/repo

# Conversation Memory
# CONVERSATION_WINDOW_SIZE=10

# Agent URL (for console app)
# AGENT_URL=http://localhost:9998

# === DATABASE CONFIGURATION ===

# Database URL - PostgreSQL for both development and production
# Local development (using Docker Compose PostgreSQL on port 5433)
DATABASE_URL=postgresql://procode_user:changeme@localhost:5433/procode

# Production (update with your production credentials)
# DATABASE_URL=postgresql://user:password@prod-host:5432/procode_db

# Database pool settings
DB_POOL_SIZE=5
DB_MAX_OVERFLOW=10

# Enable SQL query logging (useful for debugging)
SQL_ECHO=false

# PostgreSQL password (used by docker-compose.yml)
POSTGRES_PASSWORD=changeme

# === API SECURITY CONFIGURATION ===

# Enable API security (rate limiting + API key authentication)
# Set to "true" for production deployments
# ENABLE_API_SECURITY=false

# API Key for public demo access
# Generate a secure random key: openssl rand -hex 32
# DEMO_API_KEY=your-secure-api-key-here

# Rate limiting settings (requests per time window)
# RATE_LIMIT_PER_MINUTE=10
# RATE_LIMIT_PER_HOUR=100
# RATE_LIMIT_PER_DAY=1000

# CORS allowed origins (comma-separated)
# For production, restrict to your domain only
# ALLOWED_ORIGINS=https://proagent.harjadi.com
# For development, use localhost
# ALLOWED_ORIGINS=http://localhost:3000,http://localhost:3001,http://localhost:8501

# === API KEY AUTHENTICATION (Step 11) ===

# Enable API key authentication middleware
# When enabled, all non-public endpoints require valid API key
# ENABLE_API_KEY_AUTH=false

# Admin API key for managing organizations and keys
# Generate with: python -c "from security.api_key_generator import APIKeyGenerator; print(APIKeyGenerator.generate_key('live')['full_key'])"
# ADMIN_API_KEY=pk_live_your-admin-key-here

# Default rate limits for new organizations
# DEFAULT_RATE_LIMIT_PER_MINUTE=10
# DEFAULT_MONTHLY_REQUEST_LIMIT=1000
# DEFAULT_MAX_API_KEYS=2

# === EXTERNAL AGENTS CONFIGURATION ===

# Weather Agent - OpenWeatherMap API
# Sign up for free at: https://openweathermap.org/api
# Free tier: 1,000 calls/day, 60 calls/minute
OPENWEATHER_API_KEY=47399ada982fa7883b8224558c434617
# Alternative environment variable name (both work)
# WEATHER_API_KEY=47399ada982fa7883b8224558c434617
