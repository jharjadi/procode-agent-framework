# LLM Configuration - Set at least one API key to enable LLM-based intent classification
# The agent will auto-detect and use the first available provider in this order:
# 1. Ollama (Local, FREE) - Best for cost optimization
# 2. Anthropic (Claude 3 Haiku) - Cheap cloud option (12x cheaper than Sonnet)
# 3. Google (Gemini Flash-8B) - Cheap cloud option (2x cheaper than Flash)
# 4. OpenAI (GPT-4o-mini) - Already cost-optimized

# === COST-OPTIMIZED OPTIONS ===

# Option 1: Ollama (Local, FREE) - Recommended for development
# Install: brew install ollama
# Start: ollama serve
# Pull model: ollama pull llama3.2:3b
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.2:3b

# Option 2: Anthropic Claude 3 Haiku (12x cheaper than Sonnet)
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Option 3: Google Gemini Flash-8B (2x cheaper than Flash)
# GOOGLE_API_KEY=your-google-api-key-here
# GOOGLE_MODEL=gemini-1.5-flash-8b

# Option 4: OpenAI GPT-4o-mini (already cost-optimized)
# OPENAI_API_KEY=your-openai-api-key-here

# === MULTI-LLM STRATEGY SETTINGS ===

# Force specific provider for intent classification (optional)
# INTENT_LLM_PROVIDER=ollama  # or "anthropic", "google", "openai"
# LLM_PROVIDER=anthropic  # Fallback for backward compatibility

# Confidence threshold for deterministic classifier (0.0-1.0)
# Higher = fewer LLM calls = more cost savings
# CONFIDENCE_THRESHOLD=0.8  # Default: 80% confidence required

# Enable caching of intent classifications
# ENABLE_INTENT_CACHE=true

# Enable/disable LLM intent classification (default: true if any API key is set)
# USE_LLM_INTENT=true

# Tool Configuration
# Enable real GitHub API integration (default: false, uses mocked tools)
# USE_REAL_TOOLS=false
# GITHUB_TOKEN=your-github-token-here
# GITHUB_REPO=owner/repo

# Conversation Memory
# CONVERSATION_WINDOW_SIZE=10

# Agent URL (for console app)
# AGENT_URL=http://localhost:9998

# === DATABASE CONFIGURATION ===

# Database URL - PostgreSQL for both development and production
# Local development (using Docker Compose PostgreSQL on port 5433)
DATABASE_URL=postgresql://procode_user:changeme@localhost:5433/procode

# Production (update with your production credentials)
# DATABASE_URL=postgresql://user:password@prod-host:5432/procode_db

# Database pool settings
DB_POOL_SIZE=5
DB_MAX_OVERFLOW=10

# Enable SQL query logging (useful for debugging)
SQL_ECHO=false

# PostgreSQL password (used by docker-compose.yml)
POSTGRES_PASSWORD=changeme

# === API SECURITY CONFIGURATION ===

# Enable API security (rate limiting + API key authentication)
# Set to "true" for production deployments
# ENABLE_API_SECURITY=false

# API Key for public demo access
# Generate a secure random key: openssl rand -hex 32
# DEMO_API_KEY=your-secure-api-key-here

# Rate limiting settings (requests per time window)
# RATE_LIMIT_PER_MINUTE=10
# RATE_LIMIT_PER_HOUR=100
# RATE_LIMIT_PER_DAY=1000

# CORS allowed origins (comma-separated)
# For production, restrict to your domain only
# ALLOWED_ORIGINS=https://proagent.harjadi.com
# For development, use localhost
# ALLOWED_ORIGINS=http://localhost:3000,http://localhost:3001,http://localhost:8501
